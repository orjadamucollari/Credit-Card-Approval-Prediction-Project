{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n#visualization libraries\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\nimport matplotlib.pyplot as plt, numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.colors import ListedColormap\nfrom IPython.display import Image\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport plotly.offline as pyo\nfrom plotly import tools\nimport seaborn as sns\n\nimport missingno as msno #to visualize missing data\n\nfrom imblearn.over_sampling import SMOTE\nimport itertools\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix,precision_score,recall_score,roc_auc_score,f1_score,plot_confusion_matrix,plot_roc_curve,roc_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.preprocessing import LabelEncoder #label encoding for categorical columns\n\npyo.init_notebook_mode()\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-13T11:58:07.708626Z","iopub.execute_input":"2022-05-13T11:58:07.708973Z","iopub.status.idle":"2022-05-13T11:58:09.557295Z","shell.execute_reply.started":"2022-05-13T11:58:07.708884Z","shell.execute_reply":"2022-05-13T11:58:09.556363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this project, we’ll be using Credit Card Approval Dataset. The structure of our project will be as follows;\n\n* To get a basic introduction of our project & What’s the business problem associated with it ?\n* We’ll start by loading and viewing the dataset.\n* To manipulate data, if there are any missing entries in the dataset.\n* To perform exploratory data analysis (EDA) on our dataset.\n* To pre-process data before applying machine learning model to the dataset.\n* To apply machine learning models that can predict if an individual’s application for a credit card will be accepted or not.","metadata":{}},{"cell_type":"markdown","source":"**Credit Card Applications and the problems associated with it**\n\nBanks receive a lot of applications for issuance of credit cards. Many of them rejected for many reasons, like high-loan balances, low-income levels, or too many inquiries on an individual’s credit report. Manually analyzing these applications is error-prone and a time-consuming process. This task can be automated with the power of machine learning, In this project, we will be build an automatic credit card approval predictor using machine learning techniques, just like the real banks do. ","metadata":{}},{"cell_type":"markdown","source":"**Task**\n\nBuild a machine learning model to predict if an applicant is 'good' or 'bad' client, different from other tasks, the definition of 'good' or 'bad' is not given. You should use some techique, such as vintage analysis to construct you label. Also, unbalance data problem is a big problem in this task.","metadata":{}},{"cell_type":"markdown","source":"# 1. Importing Data & EDA","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"../input/credit-card-approval-prediction/application_record.csv\", encoding = 'utf-8') \nrecord = pd.read_csv(\"../input/credit-card-approval-prediction/credit_record.csv\", encoding = 'utf-8') ","metadata":{"execution":{"iopub.status.busy":"2022-05-13T11:58:14.393149Z","iopub.execute_input":"2022-05-13T11:58:14.393481Z","iopub.status.idle":"2022-05-13T11:58:15.773896Z","shell.execute_reply.started":"2022-05-13T11:58:14.39345Z","shell.execute_reply":"2022-05-13T11:58:15.77286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of datapoints for application records: {}\".format(len(data)))\nprint(\"Number of unique clients in dataset: {}\".format(len(data.ID.unique())))\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T11:58:15.776283Z","iopub.execute_input":"2022-05-13T11:58:15.776624Z","iopub.status.idle":"2022-05-13T11:58:15.837517Z","shell.execute_reply.started":"2022-05-13T11:58:15.776579Z","shell.execute_reply":"2022-05-13T11:58:15.836438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unique clients and rows are not equal,which means there are duplicates.","metadata":{}},{"cell_type":"code","source":"print(\"Number of datapoints for credit records: {}\".format(len(record)))\nprint(\"Number of unique clients in dataset: {}\".format(len(record.ID.unique())))\nrecord.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T11:58:18.170947Z","iopub.execute_input":"2022-05-13T11:58:18.171563Z","iopub.status.idle":"2022-05-13T11:58:18.210293Z","shell.execute_reply.started":"2022-05-13T11:58:18.171518Z","shell.execute_reply":"2022-05-13T11:58:18.209505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(set(record['ID']).intersection(set(data['ID']))) # checking to see how many records match in two datasets","metadata":{"execution":{"iopub.status.busy":"2022-05-13T11:58:18.609916Z","iopub.execute_input":"2022-05-13T11:58:18.610877Z","iopub.status.idle":"2022-05-13T11:58:19.093096Z","shell.execute_reply.started":"2022-05-13T11:58:18.610825Z","shell.execute_reply":"2022-05-13T11:58:19.090131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The number of unique ids in the two datasets is not equal. There are fewer customers than applications in the credit record dataset. The intersection is 36,457 customers.","metadata":{}},{"cell_type":"markdown","source":"**Missing Values**","metadata":{}},{"cell_type":"code","source":"plt_missing_1 = msno.matrix(data)\n\nplt_missing_1.set_title(\"Missing Data for application records dataset\",fontsize=20)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T19:22:50.305221Z","iopub.execute_input":"2022-05-11T19:22:50.305508Z","iopub.status.idle":"2022-05-11T19:22:53.725611Z","shell.execute_reply.started":"2022-05-11T19:22:50.305476Z","shell.execute_reply":"2022-05-11T19:22:53.724897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt_missing_2 = msno.matrix(record)\n\nplt_missing_2.set_title(\"Missing Data for credit records dataset\",fontsize=20)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T19:22:58.212364Z","iopub.execute_input":"2022-05-11T19:22:58.212638Z","iopub.status.idle":"2022-05-11T19:23:03.21769Z","shell.execute_reply.started":"2022-05-11T19:22:58.21261Z","shell.execute_reply":"2022-05-11T19:23:03.215085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have checked the null values for records data, and all good here. ","metadata":{}},{"cell_type":"markdown","source":"**Unique counts**","metadata":{}},{"cell_type":"code","source":"unique_counts = pd.DataFrame.from_records([(col, data[col].nunique()) for col in data.columns],\n                          columns=['Column_Name', 'Num_Unique']).sort_values(by=['Num_Unique'])\nunique_counts","metadata":{"execution":{"iopub.status.busy":"2022-05-11T18:11:38.958252Z","iopub.execute_input":"2022-05-11T18:11:38.958717Z","iopub.status.idle":"2022-05-11T18:11:39.320784Z","shell.execute_reply.started":"2022-05-11T18:11:38.958677Z","shell.execute_reply":"2022-05-11T18:11:39.319814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_counts = pd.DataFrame.from_records([(col, record[col].nunique()) for col in record.columns],\n                          columns=['Column_Name', 'Num_Unique']).sort_values(by=['Num_Unique'])\nunique_counts","metadata":{"execution":{"iopub.status.busy":"2022-05-11T18:11:39.322483Z","iopub.execute_input":"2022-05-11T18:11:39.322756Z","iopub.status.idle":"2022-05-11T18:11:39.424181Z","shell.execute_reply.started":"2022-05-11T18:11:39.322724Z","shell.execute_reply":"2022-05-11T18:11:39.42319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2 - Data Visualization","metadata":{}},{"cell_type":"markdown","source":"Seaborn Plot Styling","metadata":{}},{"cell_type":"code","source":"sns.set_context(\"notebook\",font_scale=.7,rc={\"grid.linewidth\": 0.1,'patch.linewidth': 0.0,\n    \"axes.grid\":True,\n    \"grid.linestyle\": \"-\",\n    \"axes.titlesize\" : 13,                                       \n    \"figure.autolayout\":True})\n                \npalette_1 = ['#FF5E5B','#EC9B9A','#00CECB','#80DE99','#C0E680','#FFED66']\n\nsns.set_palette(sns.color_palette(sns.color_palette(palette_1)))","metadata":{"execution":{"iopub.status.busy":"2022-05-11T21:10:43.732888Z","iopub.execute_input":"2022-05-11T21:10:43.733375Z","iopub.status.idle":"2022-05-11T21:10:43.742562Z","shell.execute_reply.started":"2022-05-11T21:10:43.733339Z","shell.execute_reply":"2022-05-11T21:10:43.74152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\n\ncols_to_plot = [\"CNT_CHILDREN\",\"AMT_INCOME_TOTAL\",\"DAYS_BIRTH\",\"DAYS_EMPLOYED\"]\ndata[cols_to_plot].hist(edgecolor='black', linewidth=1.2)\nfig=plt.gcf()\nfig.set_size_inches(12,6)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T20:58:08.300715Z","iopub.execute_input":"2022-05-10T20:58:08.301042Z","iopub.status.idle":"2022-05-10T20:58:09.117114Z","shell.execute_reply.started":"2022-05-10T20:58:08.301009Z","shell.execute_reply":"2022-05-10T20:58:09.115012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are outliers in 2 columns.\n\n* CNT_CHILDREN\n* AMT_INCOME_TOTAL","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1,2)\n\ng1=sns.countplot(y=data.NAME_INCOME_TYPE,linewidth=1.2, ax=axes[0])\ng1.set_title(\"Customer Distribution by Income Type\")\ng1.set_xlabel(\"Count\")\n\ng2=sns.countplot(y=data.NAME_FAMILY_STATUS,linewidth=1.2, ax=axes[1])\ng2.set_title(\"Customer Distribution by Family Status\")\ng2.set_xlabel(\"Count\")\n\nfig.set_size_inches(14,5)\n\nplt.tight_layout()\n\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T20:58:09.119392Z","iopub.execute_input":"2022-05-10T20:58:09.119775Z","iopub.status.idle":"2022-05-10T20:58:10.387339Z","shell.execute_reply.started":"2022-05-10T20:58:09.119724Z","shell.execute_reply":"2022-05-10T20:58:10.386309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1,2)\n\ng1= sns.countplot(y=data.NAME_HOUSING_TYPE,linewidth=1.2, ax=axes[0])\ng1.set_title(\"Customer Distribution by Housing Type\")\ng1.set_xlabel(\"Count\")\ng1.set_ylabel(\"Housing Type\")\n\ng2= sns.countplot(y=data.NAME_EDUCATION_TYPE, ax=axes[1])\ng2.set_title(\"Customer Distribution by Education\")\ng2.set_xlabel(\"Count\")\ng2.set_ylabel(\"Education Type\")\n\nfig.set_size_inches(14,5)\n\nplt.tight_layout()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T20:58:10.389107Z","iopub.execute_input":"2022-05-10T20:58:10.390039Z","iopub.status.idle":"2022-05-10T20:58:11.736662Z","shell.execute_reply.started":"2022-05-10T20:58:10.389989Z","shell.execute_reply":"2022-05-10T20:58:11.735763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1,3)\n\ng1= data['CODE_GENDER'].value_counts().plot.pie(explode=[0.1,0.1],autopct='%1.1f%%',shadow=True, colors=[\"#76B5B3\",\"#EC9B9A\"],textprops = {'fontsize':12}, ax=axes[0])\ng1.set_title(\"Customer Distribution by Gender\")\n\ng2= data['FLAG_OWN_CAR'].value_counts().plot.pie(explode=[0.1,0.1],autopct='%1.1f%%',shadow=True,colors=[\"#80DE99\",\"#00CECB\"],textprops = {'fontsize':12}, ax=axes[1])\ng2.set_title(\"Car Ownership\")\n\ng3= data['FLAG_OWN_REALTY'].value_counts().plot.pie(explode=[0.1,0.1],autopct='%1.1f%%',shadow=True,colors=[\"#76B5B3\",\"#00CECB\"],textprops = {'fontsize':12}, ax=axes[2])\ng3.set_title(\"Realty Ownership\")\n\nfig.set_size_inches(14,5)\n\nplt.tight_layout()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T20:58:11.738852Z","iopub.execute_input":"2022-05-10T20:58:11.740521Z","iopub.status.idle":"2022-05-10T20:58:12.383176Z","shell.execute_reply.started":"2022-05-10T20:58:11.740468Z","shell.execute_reply":"2022-05-10T20:58:12.382495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3 - Data Preprocessing & Feature Engineering","metadata":{}},{"cell_type":"code","source":"data = data.drop_duplicates('ID', keep='last') #remove duplicate values and keep the last entry of the ID if its repeated.\ndata.drop('OCCUPATION_TYPE', axis=1, inplace=True) #the occupation type has missing values, we dropped them.","metadata":{"execution":{"iopub.status.busy":"2022-05-13T11:58:25.796825Z","iopub.execute_input":"2022-05-13T11:58:25.797524Z","iopub.status.idle":"2022-05-13T11:58:25.941259Z","shell.execute_reply.started":"2022-05-13T11:58:25.797482Z","shell.execute_reply":"2022-05-13T11:58:25.940121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"object_columns = data.columns[data.dtypes =='object'].tolist() #object columns in dataset\n\nunique_counts = pd.DataFrame.from_records([(col, data[object_columns][col].nunique()) for col in data[object_columns].columns],\n                          columns=['Column_Name', 'Num_Unique']).sort_values(by=['Num_Unique'])\n\nunique_counts #unique counts for object columns ","metadata":{"execution":{"iopub.status.busy":"2022-05-13T11:58:25.943164Z","iopub.execute_input":"2022-05-13T11:58:25.943553Z","iopub.status.idle":"2022-05-13T11:58:26.400802Z","shell.execute_reply.started":"2022-05-13T11:58:25.943451Z","shell.execute_reply":"2022-05-13T11:58:26.39936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have filtered the columns that have non numeric values to see if they are useful. We will convert them numeric. ","metadata":{}},{"cell_type":"code","source":"#renaming columns\n\ndata.rename(columns={\"CODE_GENDER\":\"Gender\",\"FLAG_OWN_CAR\":\"Own_Car\",\"FLAG_OWN_REALTY\":\"Own_Realty\",\n                     \"CNT_CHILDREN\":\"Children_Count\",\"AMT_INCOME_TOTAL\":\"Income\",\"NAME_EDUCATION_TYPE\":\"Education\",\n                     \"NAME_FAMILY_STATUS\":\"Family_Status\",\"NAME_HOUSING_TYPE\":\"Housing_Type\",\"DAYS_BIRTH\":\"Birthday\",\n                     \"DAYS_EMPLOYED\":\"Employment_Date\",\"FLAG_MOBIL\":\"Own_Mobile\",\"FLAG_WORK_PHONE\":\"Own_Work_Phone\",\n                     \"FLAG_PHONE\":\"Own_Phone\",\"FLAG_EMAIL\":\"Own_Email\",\"CNT_FAM_MEMBERS\":\"Family_Member_Count\",\n                    \"NAME_INCOME_TYPE\":\"Income_Type\"},inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T11:58:26.65691Z","iopub.execute_input":"2022-05-13T11:58:26.65781Z","iopub.status.idle":"2022-05-13T11:58:26.665906Z","shell.execute_reply.started":"2022-05-13T11:58:26.657753Z","shell.execute_reply":"2022-05-13T11:58:26.664907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#all users account open month\nopen_month=pd.DataFrame(record.groupby([\"ID\"])[\"MONTHS_BALANCE\"].agg(min))\nopen_month=open_month.rename(columns={'MONTHS_BALANCE':'begin_month'}) \ncustomer_data=pd.merge(data,open_month,how=\"left\",on=\"ID\") #merge to record data\n\n#convert categoric features into numeric\n\ncustomer_data[\"Gender\"] =  customer_data['Gender'].replace(['F','M'],[0,1])\ncustomer_data[\"Own_Car\"] = customer_data[\"Own_Car\"].replace([\"Y\",\"N\"],[1,0])\ncustomer_data[\"Own_Realty\"] = customer_data[\"Own_Realty\"].replace([\"Y\",\"N\"],[1,0])\ncustomer_data[\"Is_Working\"] = customer_data[\"Income_Type\"].replace([\"Working\",\"Commercial associate\",\"State servant\",\"Pensioner\",\"Student\"],[1,1,1,0,0])\n\ncustomer_data[\"In_Relationship\"] = customer_data[\"Family_Status\"].replace([\"Civil marriage\",\"Married\",\"Single / not married\",\n                                                                          \"Separated\",\"Widow\"],[1,1,0,0,0])\n\nhousing_type = {'House / apartment' : 'House / apartment',\n                   'With parents': 'With parents',\n                    'Municipal apartment' : 'House / apartment',\n                    'Rented apartment': 'House / apartment',\n                    'Office apartment': 'House / apartment',\n                    'Co-op apartment': 'House / apartment'}\n\ncustomer_data[\"Housing_Type\"] = customer_data['Housing_Type'].map(housing_type)\n\nfamily_status = {'Single / not married':'Single',\n                     'Separated':'Single',\n                     'Widow':'Single',\n                     'Civil marriage':'Married',\n                    'Married':'Married'}\n\ncustomer_data[\"Family_Status\"] = customer_data[\"Family_Status\"].map(family_status)\n\neducation_type = {'Secondary / secondary special':'secondary',\n                     'Lower secondary':'secondary',\n                     'Higher education':'Higher education',\n                     'Incomplete higher':'Higher education',\n                     'Academic degree':'Academic degree'}\n\ncustomer_data[\"Education\"] = customer_data[\"Education\"].map(education_type)\n\nincome_type = {'Commercial associate':'Working',\n                  'State servant':'Working',\n                  'Working':'Working',\n                  'Pensioner':'Pensioner',\n                  'Student':'Student'}\n\ncustomer_data[\"Income_Type\"] = customer_data[\"Income_Type\"].map(income_type)\n\ncustomer_data[\"Household_Size\"] = customer_data[\"Children_Count\"] + customer_data[\"In_Relationship\"].apply(lambda x: 2 if x==1 else 1)\n\ncustomer_data[\"Age\"] = round((customer_data.Birthday/365)*-1)\n\ncustomer_data[\"Experience\"] = customer_data.Employment_Date/365\ncustomer_data['Experience']=customer_data['Experience'].apply(lambda v : int(v*-1) if v <0 else 0)\n\ncustomer_data=customer_data.drop(columns=['Employment_Date','Birthday','Children_Count'])\n\ncustomer_data= pd.get_dummies(customer_data, columns=['Income_Type', 'Education','Family_Status',\"Housing_Type\"])","metadata":{"execution":{"iopub.status.busy":"2022-05-13T11:58:28.676696Z","iopub.execute_input":"2022-05-13T11:58:28.677744Z","iopub.status.idle":"2022-05-13T11:58:31.51385Z","shell.execute_reply.started":"2022-05-13T11:58:28.677657Z","shell.execute_reply":"2022-05-13T11:58:31.512973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T11:58:31.515451Z","iopub.execute_input":"2022-05-13T11:58:31.515807Z","iopub.status.idle":"2022-05-13T11:58:31.53695Z","shell.execute_reply.started":"2022-05-13T11:58:31.515775Z","shell.execute_reply":"2022-05-13T11:58:31.536244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will look at numeric columns to see if there is anything that needs to be changed.","metadata":{}},{"cell_type":"code","source":"other_numerical_cols = [\"Income\",\"Age\",\"Experience\",\"Household_Size\"]\n\nfig = make_subplots(rows=2, cols=2, start_cell=\"bottom-left\",\n                   subplot_titles=(\"Income\", \"Age\", \"Experience\", \"Family Member Count\"))\n\nfig.add_trace(go.Box(x=customer_data.Income, name='Income',boxmean=True),row=1,col=1)\nfig.add_trace(go.Box(x=customer_data.Age, name='Age', boxmean=True), row=1, col=2)\nfig.add_trace(go.Box(x=customer_data.Experience, name='Experience', boxmean=True), row=2, col=1)\nfig.add_trace(go.Box(x=customer_data.Household_Size, name=\"Family Member Count\", boxmean=True),row=2, col=2)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-11T21:11:06.626078Z","iopub.execute_input":"2022-05-11T21:11:06.626377Z","iopub.status.idle":"2022-05-11T21:11:07.971724Z","shell.execute_reply.started":"2022-05-11T21:11:06.626344Z","shell.execute_reply":"2022-05-11T21:11:07.970095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As seen above, there are some outliers values in children count, family member count, income and employment rate columns\n\n* We need to remove these outliers to make sure they do not affect our model results.\n* We will now remove these outliers by using z scores.","metadata":{}},{"cell_type":"code","source":"def calculate_z_scores(df, cols):\n    for col in cols:\n        df[col+\"_z_score\"] = (df[col] - df[col].mean())/df[col].std()\n    return df\n\ndf_2 = calculate_z_scores(df = customer_data, cols = [\"Income\",\"Experience\",\"Household_Size\"])\n\n\n#removing outliers\nfilter_2 = df_2.Household_Size_z_score.abs() <= 3.5\nfilter_3 = df_2.Experience_z_score.abs() <= 3.5\nfilter_4 = df_2.Income_z_score.abs() <= 3.5\n\ncustomer_apps = df_2[filter_2 & filter_3 & filter_4]\n\ncustomer_apps.drop(columns= [\"Income_z_score\",\"Experience_z_score\",\"Household_Size_z_score\"],inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T11:58:32.452454Z","iopub.execute_input":"2022-05-13T11:58:32.45332Z","iopub.status.idle":"2022-05-13T11:58:32.582233Z","shell.execute_reply.started":"2022-05-13T11:58:32.453279Z","shell.execute_reply":"2022-05-13T11:58:32.581332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"other_numerical_cols = [\"Income\",\"Age\",\"Experience\",\"Family_Member_Count\"]\n\nfig = make_subplots(rows=2, cols=2, start_cell=\"bottom-left\",\n                   subplot_titles=(\"Income\", \"Age\", \"Experience\", \"Family Member Count\"))\n\nfig.add_trace(go.Box(x=customer_apps.Income, name='Income',boxmean=True),row=1,col=1)\nfig.add_trace(go.Box(x=customer_apps.Age, name='Age', boxmean=True), row=1, col=2)\nfig.add_trace(go.Box(x=customer_apps.Experience, name='Experience', boxmean=True), row=2, col=1)\nfig.add_trace(go.Box(x=customer_apps.Household_Size, name=\"Family Member Count\", boxmean=True),row=2, col=2)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T11:29:29.438393Z","iopub.execute_input":"2022-05-13T11:29:29.438999Z","iopub.status.idle":"2022-05-13T11:29:30.77239Z","shell.execute_reply.started":"2022-05-13T11:29:29.438956Z","shell.execute_reply":"2022-05-13T11:29:30.7711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"record['dep_value'] = None\nrecord['dep_value'][record['STATUS'] =='2']='Yes' \nrecord['dep_value'][record['STATUS'] =='3']='Yes' \nrecord['dep_value'][record['STATUS'] =='4']='Yes' \nrecord['dep_value'][record['STATUS'] =='5']='Yes' \n\nrecord_count=record.groupby('ID').count()\nrecord_count['dep_value'][record_count['dep_value'] > 0]='Yes' \nrecord_count['dep_value'][record_count['dep_value'] == 0]='No' \nrecord_count = record_count[['dep_value']]","metadata":{"execution":{"iopub.status.busy":"2022-05-13T11:58:35.226307Z","iopub.execute_input":"2022-05-13T11:58:35.226574Z","iopub.status.idle":"2022-05-13T11:58:36.19879Z","shell.execute_reply.started":"2022-05-13T11:58:35.226546Z","shell.execute_reply":"2022-05-13T11:58:36.197969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data frame to analyze length of time since initial approval of credit card\n# Shows number of past dues, paid off and no loan status.\ngrouped = record.groupby('ID')\n\npivot_tb = record.pivot(index = 'ID', columns = 'MONTHS_BALANCE', values = 'STATUS')\npivot_tb['open_month'] = grouped['MONTHS_BALANCE'].min()\npivot_tb['end_month'] = grouped['MONTHS_BALANCE'].max()\npivot_tb['window'] = pivot_tb['end_month'] - pivot_tb['open_month']\npivot_tb['window'] += 1 # Adding 1 since month starts at 0.\n\n#Counting number of past dues, paid offs and no loans.\npivot_tb['paid_off'] = pivot_tb[pivot_tb.iloc[:,0:61] == 'C'].count(axis = 1)\npivot_tb['pastdue_1-29'] = pivot_tb[pivot_tb.iloc[:,0:61] == '0'].count(axis = 1)\npivot_tb['pastdue_30-59'] = pivot_tb[pivot_tb.iloc[:,0:61] == '1'].count(axis = 1)\npivot_tb['pastdue_60-89'] = pivot_tb[pivot_tb.iloc[:,0:61] == '2'].count(axis = 1)\npivot_tb['pastdue_90-119'] = pivot_tb[pivot_tb.iloc[:,0:61] == '3'].count(axis = 1)\npivot_tb['pastdue_120-149'] = pivot_tb[pivot_tb.iloc[:,0:61] == '4'].count(axis = 1)\npivot_tb['pastdue_over_150'] = pivot_tb[pivot_tb.iloc[:,0:61] == '5'].count(axis = 1)\npivot_tb['no_loan'] = pivot_tb[pivot_tb.iloc[:,0:61] == 'X'].count(axis = 1)\n#Setting Id column to merge with app data.\npivot_tb['ID'] = pivot_tb.index","metadata":{"execution":{"iopub.status.busy":"2022-05-13T11:58:40.3991Z","iopub.execute_input":"2022-05-13T11:58:40.399832Z","iopub.status.idle":"2022-05-13T11:58:44.932239Z","shell.execute_reply.started":"2022-05-13T11:58:40.399791Z","shell.execute_reply":"2022-05-13T11:58:44.931623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pivot_tb.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T11:58:44.933489Z","iopub.execute_input":"2022-05-13T11:58:44.934024Z","iopub.status.idle":"2022-05-13T11:58:44.95724Z","shell.execute_reply.started":"2022-05-13T11:58:44.933993Z","shell.execute_reply":"2022-05-13T11:58:44.956585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = pd.DataFrame()\ntarget['ID'] = pivot_tb.index\ntarget['paid_off'] = pivot_tb['paid_off'].values\ntarget['#_of_pastdues'] = pivot_tb['pastdue_1-29'].values+ pivot_tb['pastdue_30-59'].values + pivot_tb['pastdue_60-89'].values +pivot_tb['pastdue_90-119'].values+pivot_tb['pastdue_120-149'].values +pivot_tb['pastdue_over_150'].values\ntarget['no_loan'] = pivot_tb['no_loan'].values\ncustomer_apps_1 = customer_apps.merge(target, how = 'inner', on = 'ID')\n\ncustomer_apps_2=pd.merge(customer_apps_1,record_count,how='inner',on='ID')\ncustomer_apps_2['target']=customer_apps_2['dep_value']\ncustomer_apps_2.loc[customer_apps_2['target']=='Yes','target']=1\ncustomer_apps_2.loc[customer_apps_2['target']=='No','target']=0\n\ncustomer_apps_2.drop(columns=[\"dep_value\"],inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T11:58:44.958344Z","iopub.execute_input":"2022-05-13T11:58:44.958674Z","iopub.status.idle":"2022-05-13T11:58:45.07568Z","shell.execute_reply.started":"2022-05-13T11:58:44.958646Z","shell.execute_reply":"2022-05-13T11:58:45.075034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n\nf, ax = plt.subplots(figsize=(15,15))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\ncorr = customer_apps_2.drop(columns=[\"Own_Mobile\"]).corr().round(1)\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, annot=True, mask = mask, cmap=cmap)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T20:32:49.80937Z","iopub.execute_input":"2022-05-12T20:32:49.809648Z","iopub.status.idle":"2022-05-12T20:32:50.446363Z","shell.execute_reply.started":"2022-05-12T20:32:49.809618Z","shell.execute_reply":"2022-05-12T20:32:50.445392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_apps_2['target'].value_counts().plot.pie(explode=[0.1,0.1],autopct='%1.1f%%',shadow=True, colors=['#FF5E5B', '#C0E680'],textprops = {'fontsize':7}).set_title(\"Target distribution\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T11:58:45.494121Z","iopub.execute_input":"2022-05-13T11:58:45.494559Z","iopub.status.idle":"2022-05-13T11:58:45.625368Z","shell.execute_reply.started":"2022-05-13T11:58:45.494523Z","shell.execute_reply":"2022-05-13T11:58:45.624789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_context(\"notebook\",font_scale=.7,rc={\"grid.linewidth\": 0.1,'patch.linewidth': 0.0,\n    \"axes.grid\":True,\n    \"grid.linestyle\": \"-\",\n    \"axes.titlesize\" : 13,                                       \n    'figure.figsize':(15,15)})\n                \npalette_1 = ['#FF5E5B','#EC9B9A','#00CECB','#80DE99','#C0E680','#FFED66']\n\nsns.set_palette(sns.color_palette(sns.color_palette(palette_1)))","metadata":{"execution":{"iopub.status.busy":"2022-05-12T20:33:08.844264Z","iopub.execute_input":"2022-05-12T20:33:08.845244Z","iopub.status.idle":"2022-05-12T20:33:08.852679Z","shell.execute_reply.started":"2022-05-12T20:33:08.84518Z","shell.execute_reply":"2022-05-12T20:33:08.851644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1,3)\n\ng1=sns.boxenplot(x='target', y='Income', data=customer_apps_2,palette=['#FF5E5B', '#C0E680'], ax=axes[0])\ng1.set_title(\"Income-Target\")\ng2=sns.boxenplot(x='target', y='Age', data=customer_apps_2,palette=['#FF5E5B', '#C0E680'], ax=axes[1])\ng2.set_title(\"Age-Target\")\ng3=sns.boxenplot(x='target', y='Experience', data=customer_apps_2,palette=['#FF5E5B', '#C0E680'], ax=axes[2])\ng3.set_title(\"Work Experience-Target\")\n\nfig.set_size_inches(14,5)\n\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-05-11T21:14:26.114519Z","iopub.execute_input":"2022-05-11T21:14:26.114836Z","iopub.status.idle":"2022-05-11T21:14:26.738249Z","shell.execute_reply.started":"2022-05-11T21:14:26.114796Z","shell.execute_reply":"2022-05-11T21:14:26.737301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(data=customer_apps_2, x='Income', hue=\"Is_Working\", col='target', kind=\"kde\", height=4, facet_kws={'sharey': False, 'sharex': False},palette=['#C70039','#80DE99'])\nsns.displot(data=customer_apps_2, x='Age', hue=\"Is_Working\", col='target', kind=\"kde\", height=4, facet_kws={'sharey': False, 'sharex': False},palette=['#C70039','#80DE99'])\nsns.displot(data=customer_apps_2, x='Experience', hue=\"Is_Working\", col='target', kind=\"kde\", height=4, facet_kws={'sharey': False, 'sharex': False},palette=['#C70039','#80DE99'])\nsns.displot(data=customer_apps_2, x='begin_month', hue=\"Is_Working\", col='target', kind=\"kde\", height=4, facet_kws={'sharey': False, 'sharex': False},palette=['#C70039','#80DE99'])\n","metadata":{"execution":{"iopub.status.busy":"2022-05-11T21:14:26.739878Z","iopub.execute_input":"2022-05-11T21:14:26.74013Z","iopub.status.idle":"2022-05-11T21:14:30.336219Z","shell.execute_reply.started":"2022-05-11T21:14:26.740099Z","shell.execute_reply":"2022-05-11T21:14:30.335483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(data=customer_apps_2, x='no_loan', hue=\"Is_Working\", col='target', kind=\"kde\", height=4, facet_kws={'sharey': False, 'sharex': False},palette=['#C70039','#80DE99'])\nsns.displot(data=customer_apps_2, x='#_of_pastdues', hue=\"Is_Working\", col='target', kind=\"kde\", height=4, facet_kws={'sharey': False, 'sharex': False},palette=['#C70039','#80DE99'])\nsns.displot(data=customer_apps_2, x='paid_off', hue=\"Is_Working\", col='target', kind=\"kde\", height=4, facet_kws={'sharey': False, 'sharex': False},palette=['#C70039','#80DE99'])","metadata":{"execution":{"iopub.status.busy":"2022-05-11T21:14:30.337647Z","iopub.execute_input":"2022-05-11T21:14:30.338229Z","iopub.status.idle":"2022-05-11T21:14:33.137692Z","shell.execute_reply.started":"2022-05-11T21:14:30.338196Z","shell.execute_reply":"2022-05-11T21:14:33.136722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_apps_2.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-12T21:37:38.422057Z","iopub.execute_input":"2022-05-12T21:37:38.422803Z","iopub.status.idle":"2022-05-12T21:37:38.445656Z","shell.execute_reply.started":"2022-05-12T21:37:38.422759Z","shell.execute_reply":"2022-05-12T21:37:38.445006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Feature Selection","metadata":{}},{"cell_type":"code","source":"X = customer_apps_2.iloc[:,1:-1]\ny = customer_apps_2[[\"target\"]]","metadata":{"execution":{"iopub.status.busy":"2022-05-13T11:58:52.170329Z","iopub.execute_input":"2022-05-13T11:58:52.170692Z","iopub.status.idle":"2022-05-13T11:58:52.180649Z","shell.execute_reply.started":"2022-05-13T11:58:52.170653Z","shell.execute_reply":"2022-05-13T11:58:52.179607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n#splitting data into train-test\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100, test_size=0.3)\nprint(X_train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T11:58:58.211646Z","iopub.execute_input":"2022-05-13T11:58:58.211932Z","iopub.status.idle":"2022-05-13T11:58:58.229772Z","shell.execute_reply.started":"2022-05-13T11:58:58.211898Z","shell.execute_reply":"2022-05-13T11:58:58.228852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**SMOTE (Synthetic Minority Oversampling Technique) to Balance Dataset**\n\nA problem with imbalanced classification is that there are too few examples of the minority class for a model to effectively learn the decision boundary. \n\nOne way to solve this problem is to oversample the examples in the minority class. This can be achieved by simply duplicating examples from the minority class in the training dataset prior to fitting a model. This can balance the class distribution but does not provide any additional information to the model. An improvement on duplicating examples from the minority class is to synthesize new examples from the minority class.\n\nSMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line.\n\nWe use Synthetic Minority Over-Sampling Technique(SMOTE) to overcome sample imbalance problem.","metadata":{}},{"cell_type":"markdown","source":"It's crucial that SMOTE technique was applied only train dataset.","metadata":{}},{"cell_type":"code","source":"y_train = y_train.astype('int')\nX_balance,Y_balance = SMOTE().fit_resample(X_train,y_train)\nX_balance = pd.DataFrame(X_balance, columns = X_train.columns)\nY_balance = pd.DataFrame(Y_balance, columns=[\"target\"])","metadata":{"execution":{"iopub.status.busy":"2022-05-13T11:58:59.363181Z","iopub.execute_input":"2022-05-13T11:58:59.363772Z","iopub.status.idle":"2022-05-13T11:58:59.454055Z","shell.execute_reply.started":"2022-05-13T11:58:59.363736Z","shell.execute_reply":"2022-05-13T11:58:59.452955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Calculate Information Value**","metadata":{}},{"cell_type":"markdown","source":"The weight of evidence tells the predictive power of an independent variable in relation to the dependent variable.\n\nThe weight of evidence tells the predictive power of a single feature concerning its independent feature. If any of the categories/bins of a feature has a large proportion of events compared to the proportion of non-events, we will get a high value of WoE which in turn says that that class of the feature separates the events from non-events.","metadata":{}},{"cell_type":"code","source":"# data size check\nlen(X_balance) == len(X_balance)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T11:59:02.402492Z","iopub.execute_input":"2022-05-13T11:59:02.402933Z","iopub.status.idle":"2022-05-13T11:59:02.409185Z","shell.execute_reply.started":"2022-05-13T11:59:02.402885Z","shell.execute_reply":"2022-05-13T11:59:02.408339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate information value\ndef calc_iv(df, feature, target, pr=False):\n    \n    lst = []\n\n    df[feature] = df[feature].fillna(\"NULL\")\n\n    for i in range(df[feature].nunique()):\n        val = list(df[feature].unique())[i]\n        lst.append([feature,                                                        # Variable\n                    val,                                                            # Value\n                    df[df[feature] == val].count()[feature],                        # All\n                    df[(df[feature] == val) & (df[target] == 0)].count()[feature],  # Good (think: Fraud == 0)\n                    df[(df[feature] == val) & (df[target] == 1)].count()[feature]]) # Bad (think: Fraud == 1)\n\n    data = pd.DataFrame(lst, columns=['Variable', 'Value', 'All', 'Good', 'Bad'])\n\n    data['Share'] = data['All'] / data['All'].sum()\n    data['Bad Rate'] = data['Bad'] / data['All']\n    data['Distribution Good'] = (data['All'] - data['Bad']) / (data['All'].sum() - data['Bad'].sum())\n    data['Distribution Bad'] = data['Bad'] / data['Bad'].sum()\n    data['WoE'] = np.log(data['Distribution Good'] / data['Distribution Bad'])\n\n    data = data.replace({'WoE': {np.inf: 0, -np.inf: 0}})\n\n    data['IV'] = data['WoE'] * (data['Distribution Good'] - data['Distribution Bad'])\n\n    data = data.sort_values(by=['Variable', 'Value'], ascending=[True, True])\n    data.index = range(len(data.index))\n\n    if pr:\n        print(data)\n        print('IV = ', data['IV'].sum())\n\n    iv = data['IV'].sum()\n\n    return iv, data","metadata":{"execution":{"iopub.status.busy":"2022-05-12T19:14:54.508337Z","iopub.execute_input":"2022-05-12T19:14:54.508805Z","iopub.status.idle":"2022-05-12T19:14:54.523327Z","shell.execute_reply.started":"2022-05-12T19:14:54.508747Z","shell.execute_reply":"2022-05-12T19:14:54.522494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"iv_df = X_balance.copy()\niv_df[\"target\"] = y_train\n\nfeatures = iv_df.columns[:-1].tolist()\n\niv_list = []\nfor feature in features:\n    iv, data = calc_iv(iv_df, feature, 'target')\n    iv_list.append(round(iv,4))\n\nwoe_df = pd.DataFrame(np.column_stack([features, iv_list]), \n                      columns=['Feature', 'iv'])\nwoe_df","metadata":{"execution":{"iopub.status.busy":"2022-05-12T17:09:48.146847Z","iopub.execute_input":"2022-05-12T17:09:48.147155Z","iopub.status.idle":"2022-05-12T17:18:34.489509Z","shell.execute_reply.started":"2022-05-12T17:09:48.147119Z","shell.execute_reply":"2022-05-12T17:18:34.488826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Feature Scaling**","metadata":{}},{"cell_type":"markdown","source":"Feature scaling is essential for machine learning algorithms that calculate distances between data. The ML algorithm is sensitive to the “relative scales of features,” which usually happens when it uses the numeric values of the features rather than say their rank.In many algorithms, when we desire faster convergence, scaling is a must.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(X_balance)\n\nX_train = pd.DataFrame(scaler.transform(X_balance), columns=[X_balance.columns])","metadata":{"execution":{"iopub.status.busy":"2022-05-12T17:18:46.616868Z","iopub.execute_input":"2022-05-12T17:18:46.617192Z","iopub.status.idle":"2022-05-12T17:18:46.648247Z","shell.execute_reply.started":"2022-05-12T17:18:46.617156Z","shell.execute_reply":"2022-05-12T17:18:46.647154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n","metadata":{}},{"cell_type":"markdown","source":"We notice in the value counts above that label types are now balanced\nthe problem of oversampling is solved now\nwe will now implement different models to see which one performs the best","metadata":{}},{"cell_type":"markdown","source":"**RFE (Recursive Feature Elimination)**","metadata":{}},{"cell_type":"markdown","source":"Recursive feature elimination (RFE) is a feature selection method that fits a model and removes the weakest feature (or features) until the specified number of features is reached. Features are ranked by the model’s coef_ or feature_importances_ attributes, and by recursively eliminating a small number of features per loop, RFE attempts to eliminate dependencies and collinearity that may exist in the model.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(solver='liblinear')\nrfe = RFE(model, 15)\nfit = rfe.fit(X_train, Y_balance)\nrfe_features = pd.DataFrame({\"Feature\":features,\n              \"Support_LogisticRegression\":fit.support_,\n              \"Feature_Rank_logisticRegression\":fit.ranking_})\nrfe_features","metadata":{"execution":{"iopub.status.busy":"2022-05-12T17:18:53.292943Z","iopub.execute_input":"2022-05-12T17:18:53.293715Z","iopub.status.idle":"2022-05-12T17:19:01.942561Z","shell.execute_reply.started":"2022-05-12T17:18:53.293669Z","shell.execute_reply":"2022-05-12T17:19:01.941612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ExtraTreesClassifier**\n\nThe purpose of the ExtraTreesClassifier is to fit a number of randomized decision trees to the data, and in this regard is a from of ensemble learning. Particularly, random splits of all observations are carried out to ensure that the model does not overfit the data.\n\nEach Decision Tree in the Extra Trees Forest is constructed from the original training sample. Then, at each test node, Each tree is provided with a random sample of k features from the feature-set from which each decision tree must select the best feature to split the data based on some mathematical criteria (typically the Gini Index). This random sample of features leads to the creation of multiple de-correlated decision trees.\n\n","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\n\nmodel = ExtraTreesClassifier(n_estimators=10)\nmodel.fit(X_balance, Y_balance)\nfeature_importances = pd.DataFrame({\"Feature\":features,\n              \"Feature_Importance_ExtratreeClassifier\":model.feature_importances_})","metadata":{"execution":{"iopub.status.busy":"2022-05-12T17:19:07.66888Z","iopub.execute_input":"2022-05-12T17:19:07.669187Z","iopub.status.idle":"2022-05-12T17:19:08.167981Z","shell.execute_reply.started":"2022-05-12T17:19:07.66915Z","shell.execute_reply":"2022-05-12T17:19:08.166722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will merge all importance scores from different feature selection methods","metadata":{}},{"cell_type":"code","source":"df1=pd.merge(woe_df, feature_importances, on=[\"Feature\"])\nfeature_selection_df = pd.merge(df1, rfe_features, on=[\"Feature\"])\nfeature_selection_df.sort_values(by=\"iv\",ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T17:19:10.518279Z","iopub.execute_input":"2022-05-12T17:19:10.519035Z","iopub.status.idle":"2022-05-12T17:19:10.549094Z","shell.execute_reply.started":"2022-05-12T17:19:10.518975Z","shell.execute_reply":"2022-05-12T17:19:10.547998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following attributes were selected according to the table above.","metadata":{}},{"cell_type":"markdown","source":"# 5. Modelling","metadata":{}},{"cell_type":"markdown","source":"'paid_off', '#_of_pastdues' and 'no_loan' features not included to modelling. For example if we know what #_of_pastdues=0 then we also know with complete certainty that the target=0. These 3 features did not be included in the model because they were used to construct the target. Other features used in modeling were selected according to the common results of different feature selection methods.","metadata":{}},{"cell_type":"code","source":"selected_features = [\"begin_month\",\"Income\",\"Experience\",\"In_Relationship\",\n                     \"Education_Higher education\",\"Education_secondary\",\"Own_Realty\",\n                     \"Family_Status_Single\",\"Family_Member_Count\",\"Is_Working\",\n                     \"Own_Car\",\"Age\"]","metadata":{"execution":{"iopub.status.busy":"2022-05-13T11:59:19.644745Z","iopub.execute_input":"2022-05-13T11:59:19.645012Z","iopub.status.idle":"2022-05-13T11:59:19.650349Z","shell.execute_reply.started":"2022-05-13T11:59:19.644983Z","shell.execute_reply":"2022-05-13T11:59:19.649101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Logistic Regression, K-Nearest Neighbors, Support Vector Machine (SVM), Decision Tree, Random Forest, XGBoost and CatBoost algorithms performed.\n\nTo briefly mention these algorithms,\n\n**Logistic Regression** \nUnlike regression which uses Least Squares, the model uses Maximum Likelihood to fit a sigmoid-curve on the target variable distribution. It uses a logistic function, and most commonly used when the data in question has binary output.\n\n**K-Nearest Neighbors**\nK-Nearest Neighbor (KNN) algorithm predicts based on the specified number (k) of the nearest neighboring data points. Here, the pre-processing of the data is significant as it impacts the distance measurements directly. Unlike others, the model does not have a mathematical formula, neither any descriptive ability. \n\n**Decision Tree**\nIn this method a set of training examples is broken down into smaller and smaller subsets while at the same time an associated decision tree get incrementally developed. At the end of the learning process, a decision tree covering the training set is returned.\n\n**Random Forest**\nA Random Forest is a reliable ensemble of multiple Decision Trees (or CARTs); though more popular for classification, than regression applications. Here, the individual trees are built via bagging (i.e. aggregation of bootstraps which are nothing but multiple train datasets created via sampling of records with replacement) and split using fewer features. The resulting diverse forest of uncorrelated trees exhibits reduced variance; therefore, is more robust towards change in data and carries its prediction accuracy to new data. It works well with both continuous & categorical data.\n\n**XGBoost**\nIt is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. Execution speed and high performance are the main reasons to use XGBoost.\n\n**CatBoost**\nCatBoost is an open source algorithm based on gradient boosted decision trees. It supports numerical, categorical and text features. It works well with heterogeneous data and even relatively small data.","metadata":{}},{"cell_type":"code","source":"classifiers = {\n    \"LogisticRegression\" : LogisticRegression(),\n    \"KNeighbors\" : KNeighborsClassifier(),\n    \"DecisionTree\" : DecisionTreeClassifier(),\n    \"RandomForest\" : RandomForestClassifier(n_estimators=250,max_depth=12,min_samples_leaf=16),\n    \"XGBoost\" : XGBClassifier(max_depth=12,\n                              n_estimators=250,\n                              min_child_weight=8, \n                              subsample=0.8, \n                              learning_rate =0.02,    \n                              seed=42),\n    \"CatBoost\" : CatBoostClassifier(iterations=250,\n                           learning_rate=0.2,\n                           od_type='Iter',\n                           verbose=25,\n                           depth=16,\n                           random_seed=42)\n}\n\nresult_table = pd.DataFrame(columns=['classifiers','accuracy','presicion','recall','f1_score','fpr','tpr','auc'])\n\ny_test = y_test.astype(int)\n\n\nfor key, classifier in classifiers.items():\n    classifier.fit(X_balance[selected_features], Y_balance)\n    y_predict = classifier.predict(X_test[selected_features])\n    \n    yproba = classifier.predict_proba(X_test[selected_features])[::,1]\n    \n    print(\"YAŞASIN SÜTYENLERİN BAĞIMSIZLIK MÜCADELESİ\")\n    \n    fpr, tpr, _ = roc_curve(y_test,  yproba)\n    auc = roc_auc_score(y_test, yproba)\n    \n    conf_matrix = confusion_matrix(y_test,y_predict)\n    \n    result_table = result_table.append({'classifiers':key,\n                                        'accuracy':accuracy_score(y_test, y_predict),\n                                        'presicion':precision_score(y_test, y_predict, average='weighted'),\n                                        'recall':recall_score(y_test, y_predict, average='weighted'),\n                                        'f1_score':f1_score(y_test, y_predict, average='weighted'),\n                                        'fpr':fpr, \n                                        'tpr':tpr, \n                                        'auc':auc\n                                         }, ignore_index=True)\n        \nresult_table.set_index('classifiers', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T11:59:24.536402Z","iopub.execute_input":"2022-05-13T11:59:24.536658Z","iopub.status.idle":"2022-05-13T12:08:46.925736Z","shell.execute_reply.started":"2022-05-13T11:59:24.536631Z","shell.execute_reply":"2022-05-13T12:08:46.924663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Results","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15,10))\n\nfor cls, ax in zip(list(classifiers.values()), axes.flatten()):\n    plot_confusion_matrix(cls, \n                          X_test[selected_features], \n                          y_test, \n                          ax=ax, \n                          cmap='Blues')\n    ax.title.set_text(type(cls).__name__)\nplt.tight_layout()  \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T12:10:16.390609Z","iopub.execute_input":"2022-05-13T12:10:16.391541Z","iopub.status.idle":"2022-05-13T12:10:18.715677Z","shell.execute_reply.started":"2022-05-13T12:10:16.3915Z","shell.execute_reply":"2022-05-13T12:10:18.714813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(8,6))\n\nfor i in result_table.index:\n    plt.plot(result_table.loc[i]['fpr'], \n             result_table.loc[i]['tpr'], \n             label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['auc']))\n    \nplt.plot([0,1], [0,1], color='orange', linestyle='--')\n\nplt.xticks(np.arange(0.0, 1.1, step=0.1))\nplt.xlabel(\"Flase Positive Rate\", fontsize=15)\n\nplt.yticks(np.arange(0.0, 1.1, step=0.1))\nplt.ylabel(\"True Positive Rate\", fontsize=15)\n\nplt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\nplt.legend(prop={'size':13}, loc='lower right')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T12:10:30.049092Z","iopub.execute_input":"2022-05-13T12:10:30.050152Z","iopub.status.idle":"2022-05-13T12:10:30.306233Z","shell.execute_reply.started":"2022-05-13T12:10:30.050096Z","shell.execute_reply":"2022-05-13T12:10:30.305618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_table.iloc[:,:4]","metadata":{"execution":{"iopub.status.busy":"2022-05-13T12:10:43.733993Z","iopub.execute_input":"2022-05-13T12:10:43.734281Z","iopub.status.idle":"2022-05-13T12:10:43.748163Z","shell.execute_reply.started":"2022-05-13T12:10:43.734237Z","shell.execute_reply":"2022-05-13T12:10:43.746991Z"},"trusted":true},"execution_count":null,"outputs":[]}]}